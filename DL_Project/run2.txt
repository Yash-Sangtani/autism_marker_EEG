Advanced Model Comparison (Non-Exhaustive Non-Noisy Features)

2024-12-08 17:04:04,978 - INFO - Training SVM...
2024-12-08 17:04:04,979 - INFO - Using previously saved best parameters for SVM: {'C': 100.0, 'class_weight': 'balanced', 'gamma': 'scale', 'kernel': 'rbf'}
2024-12-08 17:04:05,020 - INFO - Classification Report for SVM:
              precision    recall  f1-score   support

           0       0.60      0.92      0.73        26
           1       0.95      0.69      0.80        52

    accuracy                           0.77        78
   macro avg       0.77      0.81      0.76        78
weighted avg       0.83      0.77      0.78        78

2024-12-08 17:04:05,020 - INFO - Confusion Matrix:
[[24  2]
 [16 36]]
2024-12-08 17:04:05,020 - INFO - Training KNN...
2024-12-08 17:04:05,020 - INFO - Using previously saved best parameters for KNN: {'algorithm': 'auto', 'n_neighbors': 3, 'weights': 'distance'}
2024-12-08 17:04:05,033 - INFO - Classification Report for KNN:
              precision    recall  f1-score   support

           0       0.61      0.73      0.67        26
           1       0.85      0.77      0.81        52

    accuracy                           0.76        78
   macro avg       0.73      0.75      0.74        78
weighted avg       0.77      0.76      0.76        78

2024-12-08 17:04:05,033 - INFO - Confusion Matrix:
[[19  7]
 [12 40]]
2024-12-08 17:04:05,033 - INFO - Training GradientBoosting...
2024-12-08 17:04:05,034 - INFO - Using previously saved best parameters for GradientBoosting: {'learning_rate': 0.1, 'max_depth': 7, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 300, 'subsample': 0.5}
2024-12-08 17:04:05,443 - INFO - Classification Report for GradientBoosting:
              precision    recall  f1-score   support

           0       0.73      0.85      0.79        26
           1       0.92      0.85      0.88        52

    accuracy                           0.85        78
   macro avg       0.82      0.85      0.83        78
weighted avg       0.86      0.85      0.85        78

2024-12-08 17:04:05,443 - INFO - Confusion Matrix:
[[22  4]
 [ 8 44]]
2024-12-08 17:04:05,443 - INFO - Training ExtraTrees...
2024-12-08 17:04:05,444 - INFO - Using previously saved best parameters for ExtraTrees: {'max_depth': 30, 'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 200}
2024-12-08 17:04:05,624 - INFO - Classification Report for ExtraTrees:
              precision    recall  f1-score   support

           0       0.76      0.85      0.80        26
           1       0.92      0.87      0.89        52

    accuracy                           0.86        78
   macro avg       0.84      0.86      0.85        78
weighted avg       0.87      0.86      0.86        78

2024-12-08 17:04:05,624 - INFO - Confusion Matrix:
[[22  4]
 [ 7 45]]
2024-12-08 17:04:05,624 - INFO - Training RandomForest...
2024-12-08 17:04:05,625 - INFO - Using previously saved best parameters for RandomForest: {'bootstrap': False, 'max_depth': None, 'max_features': 'auto', 'min_samples_split': 2, 'n_estimators': 50}
C:\Users\Dhruv\anaconda3\envs\mlpr\lib\site-packages\sklearn\ensemble\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
2024-12-08 17:04:05,697 - INFO - Classification Report for RandomForest:
              precision    recall  f1-score   support

           0       0.61      0.73      0.67        26
           1       0.85      0.77      0.81        52

    accuracy                           0.76        78
   macro avg       0.73      0.75      0.74        78
weighted avg       0.77      0.76      0.76        78

2024-12-08 17:04:05,698 - INFO - Confusion Matrix:
[[19  7]
 [12 40]]
2024-12-08 17:04:05,698 - INFO - Training DecisionTree...
2024-12-08 17:04:05,698 - INFO - Using previously saved best parameters for DecisionTree: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2}
2024-12-08 17:04:05,715 - INFO - Classification Report for DecisionTree:
              precision    recall  f1-score   support

           0       0.59      0.62      0.60        26
           1       0.80      0.79      0.80        52

    accuracy                           0.73        78
   macro avg       0.70      0.70      0.70        78
weighted avg       0.73      0.73      0.73        78

2024-12-08 17:04:05,716 - INFO - Confusion Matrix:
[[16 10]
 [11 41]]
2024-12-08 17:04:05,716 - INFO - Training LogisticRegression...
2024-12-08 17:04:05,716 - INFO - Using previously saved best parameters for LogisticRegression: {'C': 10.0, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}
2024-12-08 17:04:05,732 - INFO - Classification Report for LogisticRegression:
              precision    recall  f1-score   support

           0       0.40      0.46      0.43        26
           1       0.71      0.65      0.68        52

    accuracy                           0.59        78
   macro avg       0.55      0.56      0.55        78
weighted avg       0.61      0.59      0.60        78

2024-12-08 17:04:05,733 - INFO - Confusion Matrix:
[[12 14]
 [18 34]]
2024-12-08 17:04:05,733 - INFO - Training AdaBoost...
2024-12-08 17:04:05,733 - INFO - Using previously saved best parameters for AdaBoost: {'algorithm': 'SAMME', 'learning_rate': 1.0, 'n_estimators': 200}
2024-12-08 17:04:06,021 - INFO - Classification Report for AdaBoost:
              precision    recall  f1-score   support

           0       0.56      0.73      0.63        26
           1       0.84      0.71      0.77        52

    accuracy                           0.72        78
   macro avg       0.70      0.72      0.70        78
weighted avg       0.75      0.72      0.72        78

2024-12-08 17:04:06,021 - INFO - Confusion Matrix:
[[19  7]
 [15 37]]
2024-12-08 17:04:06,021 - INFO - Training MLP...
2024-12-08 17:04:06,022 - INFO - Using previously saved best parameters for MLP: {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': [100, 50], 'learning_rate': 'constant', 'solver': 'lbfgs'}
C:\Users\Dhruv\anaconda3\envs\mlpr\lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)
2024-12-08 17:04:06,892 - INFO - Classification Report for MLP:
              precision    recall  f1-score   support

           0       0.75      0.92      0.83        26
           1       0.96      0.85      0.90        52

    accuracy                           0.87        78
   macro avg       0.85      0.88      0.86        78
weighted avg       0.89      0.87      0.87        78

2024-12-08 17:04:06,893 - INFO - Confusion Matrix:
[[24  2]
 [ 8 44]]
2024-12-08 17:04:06,893 - INFO - Training NaiveBayes...
2024-12-08 17:04:06,908 - INFO - Classification Report for NaiveBayes:
              precision    recall  f1-score   support

           0       0.37      0.88      0.52        26
           1       0.80      0.23      0.36        52

    accuracy                           0.45        78
   macro avg       0.58      0.56      0.44        78
weighted avg       0.66      0.45      0.41        78

2024-12-08 17:04:06,909 - INFO - Confusion Matrix:
[[23  3]
 [40 12]]
2024-12-08 17:04:06,909 - INFO -
Model Performance Comparison:

2024-12-08 17:04:06,910 - INFO -                 Model  Accuracy  ROC AUC Score
8                 MLP  0.871795       0.923817
2    GradientBoosting  0.846154       0.914201
3          ExtraTrees  0.858974       0.907175
1                 KNN  0.756410       0.868713
4        RandomForest  0.756410       0.861317
7            AdaBoost  0.717949       0.845414
0                 SVM  0.769231       0.841716
5        DecisionTree  0.730769       0.701923
6  LogisticRegression  0.589744       0.574704
9          NaiveBayes  0.448718       0.560651